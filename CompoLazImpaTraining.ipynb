{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LazImpaCompoNCStandard2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NlBJZf866mJ8",
        "5w2khG5Q2vkr",
        "DpFDy2Fw6prA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUrWuT5Jz1eB"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28KySsAR0Zck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1f61c5-7104-459c-eb8d-a676f96f8467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMP_NdgRz6MR"
      },
      "source": [
        "# Setup training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyOHf9BLN0dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ce52c1-24ba-447b-a459-660c441fe620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Lazimpa'...\n",
            "remote: Enumerating objects: 2453, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 2453 (delta 7), reused 10 (delta 4), pack-reused 2436\u001b[K\n",
            "Receiving objects: 100% (2453/2453), 39.15 MiB | 3.26 MiB/s, done.\n",
            "Resolving deltas: 100% (1527/1527), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "! git clone https://github.com/Daetheys/Lazimpa.git\n",
        "! mv \"./Lazimpa/egg\" \"./egg\"\n",
        "! mv \"./Lazimpa/example\" \"./example\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5qjFdU4caPN"
      },
      "outputs": [],
      "source": [
        "! mkdir -p dir_save/{accuracy,messages,sender,receiver}\n",
        "! mkdir analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVPFp7cbkdop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab82e63-e62e-42cf-a880-7250a45c763e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLPMVA'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 104 (delta 0), reused 0 (delta 0), pack-reused 103\u001b[K\n",
            "Receiving objects: 100% (104/104), 129.69 MiB | 5.19 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "Checking out files: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Daetheys/NLPMVA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution generation"
      ],
      "metadata": {
        "id": "NlBJZf866mJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuAel3U6klwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08cca419-040b-4785-bade-6a920d1ba42c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import numpy as np\n",
        "distribution = []\n",
        "with open('NLPMVA/distribution (corrected).txt') as f:\n",
        "    try:\n",
        "        while True:\n",
        "            data = f.readline()\n",
        "            distribution.append(eval(data))\n",
        "    except SyntaxError:\n",
        "        pass\n",
        "distribution_proba = np.array([d[1] for d in distribution])\n",
        "distribution_lbl = np.array([d[0] for d in distribution])\n",
        "distribution_lbl.max()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import egg.core as core\n",
        "#from scipy.stats import entropy\n",
        "from egg.core import EarlyStopperAccuracy\n",
        "from egg.zoo.channel.features import OneHotLoader, UniformLoader, OneHotLoaderCompositionality, TestLoaderCompositionality\n",
        "from egg.zoo.channel.archs import Sender, Receiver\n",
        "from egg.core.reinforce_wrappers import RnnReceiverImpatient, RnnReceiverImpatientCompositionality, RnnReceiverCompositionality\n",
        "from egg.core.reinforce_wrappers import SenderImpatientReceiverRnnReinforce, CompositionalitySenderImpatientReceiverRnnReinforce, CompositionalitySenderReceiverRnnReinforce\n",
        "from egg.core.util import dump_sender_receiver_impatient, dump_sender_receiver_impatient_compositionality, dump_sender_receiver_compositionality\n",
        "\n",
        "from egg.core.trainers import CompoTrainer\n",
        "from egg.zoo.channel.train_compositionality import *"
      ],
      "metadata": {
        "id": "kUSEeorNNnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw7Wtmgm9G4w"
      },
      "outputs": [],
      "source": [
        "class _DistributionIterator:\n",
        "    \"\"\"\n",
        "    >>> it_1 = _OneHotIterator(n_features=128, n_batches_per_epoch=2, batch_size=64, probs=np.ones(128)/128, seed=1)\n",
        "    >>> it_2 = _OneHotIterator(n_features=128, n_batches_per_epoch=2, batch_size=64, probs=np.ones(128)/128, seed=1)\n",
        "    >>> list(it_1)[0][0].allclose(list(it_2)[0][0])\n",
        "    True\n",
        "    >>> it = _OneHotIterator(n_features=8, n_batches_per_epoch=1, batch_size=4, probs=np.ones(8)/8)\n",
        "    >>> data = list(it)\n",
        "    >>> len(data)\n",
        "    1\n",
        "    >>> batch = data[0]\n",
        "    >>> x, y = batch\n",
        "    >>> x.size()\n",
        "    torch.Size([4, 8])\n",
        "    >>> x.sum(dim=1)\n",
        "    tensor([1., 1., 1., 1.])\n",
        "    >>> probs = np.zeros(128)\n",
        "    >>> probs[0] = probs[1] = 0.5\n",
        "    >>> it = _OneHotIterator(n_features=128, n_batches_per_epoch=1, batch_size=256, probs=probs, seed=1)\n",
        "    >>> batch = list(it)[0][0]\n",
        "    >>> batch[:, 0:2].sum().item()\n",
        "    256.0\n",
        "    >>> batch[:, 2:].sum().item()\n",
        "    0.0\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_batches_per_epoch, batch_size, seed=None):\n",
        "        self.n_batches_per_epoch = n_batches_per_epoch\n",
        "        self.n_features = n_features\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.batches_generated = 0\n",
        "        self.random_state = np.random.RandomState(seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.batches_generated >= self.n_batches_per_epoch:\n",
        "            raise StopIteration()\n",
        "\n",
        "        idxs = np.random.choice(len(distribution),self.batch_size,p=distribution_proba)\n",
        "        batch_data = np.zeros((self.batch_size,self.n_features*2,))\n",
        "        \n",
        "        for i in range(distribution_lbl.shape[1]):\n",
        "            batch_data[np.arange(self.batch_size),distribution_lbl[idxs,i]+i*self.n_features] = 1\n",
        "\n",
        "        self.batches_generated += 1\n",
        "        return torch.from_numpy(batch_data).float(), torch.zeros(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuzdgIUX89bX"
      },
      "outputs": [],
      "source": [
        "class DistributionLoader(torch.utils.data.DataLoader):\n",
        "    \"\"\"\n",
        "    >>> probs = np.ones(8) / 8\n",
        "    >>> data_loader = OneHotLoader(n_features=8, batches_per_epoch=3, batch_size=2, probs=probs, seed=1)\n",
        "    >>> epoch_1 = []\n",
        "    >>> for batch in data_loader:\n",
        "    ...     epoch_1.append(batch)\n",
        "    >>> [b[0].size() for b in epoch_1]\n",
        "    [torch.Size([2, 8]), torch.Size([2, 8]), torch.Size([2, 8])]\n",
        "    >>> data_loader_other = OneHotLoader(n_features=8, batches_per_epoch=3, batch_size=2, probs=probs)\n",
        "    >>> all_equal = True\n",
        "    >>> for a, b in zip(data_loader, data_loader_other):\n",
        "    ...     all_equal = all_equal and (a[0] == b[0]).all()\n",
        "    >>> all_equal.item()\n",
        "    0\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, batches_per_epoch, batch_size, seed=None):\n",
        "        self.seed = seed\n",
        "        self.batches_per_epoch = batches_per_epoch\n",
        "        self.n_features = n_features\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.seed is None:\n",
        "            seed = np.random.randint(0, 2 ** 32)\n",
        "        else:\n",
        "            seed = self.seed\n",
        "\n",
        "        return _DistributionIterator(n_features=self.n_features, n_batches_per_epoch=self.batches_per_epoch,\n",
        "                               batch_size=self.batch_size,  seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributionUniformLoader(torch.utils.data.DataLoader):\n",
        "    def __init__(self, n_features):\n",
        "        idxs = np.arange(len(distribution))\n",
        "        batch_data = np.zeros((len(distribution),n_features*2,))\n",
        "        \n",
        "        for i in range(distribution_lbl.shape[1]):\n",
        "            batch_data[np.arange(len(distribution)),distribution_lbl[idxs,i]+i*n_features] = 1\n",
        "\n",
        "        self.batch = torch.from_numpy(batch_data).float(), torch.zeros(1)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter([self.batch])"
      ],
      "metadata": {
        "id": "v4uxW2Jk9ntt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLP5u8MIz39y"
      },
      "source": [
        "#Define parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPrPgQwYKqB6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "class Config:\n",
        "\n",
        "    #IMPORTANT PARAMETERS\n",
        "    impatient = False #Impatient Listener\n",
        "    reg = False #Lazy Speaker\n",
        "    random_seed = np.random.randint(0,100) #Seed used for the training\n",
        "    lr = 3e-4 #Learning rate of both neural networks\n",
        "    batch_size = 512 #Batch size for the training\n",
        "    n_epochs = 51 #Nb of epochs for the training\n",
        "    batches_per_epoch = 1000 #Nb of batch per epoch\n",
        "\n",
        "    vocab_size = 100 #Number of words in the speaker's vocabulary\n",
        "    max_len = 2 #Maximum number of words the speaker can use to describe the combination of concepts given as input\n",
        "    n_values = 100 #Nb of different concepts\n",
        "    n_attributes = 2 #Nb of concepts that will be combined (and need to be guessed at the same time)\n",
        "\n",
        "    custom_dist = False #Use Eva's distribution instead of the one of the paper\n",
        "\n",
        "    #Eva's parameters\n",
        "    if custom_dist:\n",
        "        n_values = distribution_lbl.max()+1\n",
        "        n_attributes = 2\n",
        "\n",
        "    #LESS IMPORTANT PARAMETERS\n",
        "    n_features = 100\n",
        "\n",
        "    receiver_hidden = 900\n",
        "    receiver_num_layers = 1\n",
        "    receiver_num_heads = 1\n",
        "    receiver_embedding = 100\n",
        "    receiver_cell = 'gru'\n",
        "    receiver_entropy_coeff = 0.1\n",
        "\n",
        "    sender_hidden = 600\n",
        "    sender_num_layers = 1\n",
        "    sender_num_heads = 1\n",
        "    sender_embedding = 100\n",
        "    sender_cell = 'lstm'\n",
        "    sender_entropy_coeff = 1.3\n",
        "\n",
        "    length_cost = 0.\n",
        "    name = 'model'\n",
        "    early_stopping_thr = 0.99\n",
        "\n",
        "    dir_save = 'dir_save'#'expe_'+str(time.time()).split('.')[0]\n",
        "    checkpoint_dir = None#os.path.join('expe_'+str(time.time()).split('.')[0],'checkpoint')\n",
        "\n",
        "    unigram_pen = 0.\n",
        "\n",
        "    force_eos = False\n",
        "\n",
        "    optimizer_class = torch.optim.Adam\n",
        "    validation_freq = 1\n",
        "    device = 'cuda:0'\n",
        "\n",
        "    load_from_checkpoint = None\n",
        "    checkpoint_freq = 0\n",
        "    preemptable = False\n",
        "\n",
        "    probs = 'uniform'\n",
        "    probs_attributes = 'uniform'\n",
        "\n",
        "    att_weights = [1,1,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dump compo"
      ],
      "metadata": {
        "id": "5w2khG5Q2vkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_compositionality(game, n_attributes, n_values, device, gs_mode,epoch):\n",
        "    one_hots = torch.eye(n_values)\n",
        "\n",
        "    val=np.arange(n_values)\n",
        "    combination=list(itertools.product(val,repeat=n_attributes))\n",
        "\n",
        "    dataset=[]\n",
        "\n",
        "    for i in range(len(combination)):\n",
        "      new_input=torch.zeros(0)\n",
        "      for j in combination[i]:\n",
        "        new_input=torch.cat((new_input,one_hots[j]))\n",
        "      dataset.append(new_input)\n",
        "\n",
        "    dataset=torch.stack(dataset)\n",
        "\n",
        "    dataset=[[dataset,None]]\n",
        "\n",
        "    sender_inputs, messages, receiver_inputs, receiver_outputs, _ = \\\n",
        "        dump_sender_receiver_compositionality(game, dataset, gs=gs_mode, device=device, variable_length=True)\n",
        "\n",
        "    unif_acc = 0.\n",
        "    acc_vec=np.zeros(((n_values**n_attributes), n_attributes))\n",
        "\n",
        "    for i in range(len(receiver_outputs)):\n",
        "      message=messages[i]\n",
        "      correct=True\n",
        "      if i<n_values**n_attributes:\n",
        "          for j in range(len(list(combination[i]))):\n",
        "            if receiver_outputs[i][j]==list(combination[i])[j]:\n",
        "              unif_acc+=1\n",
        "              acc_vec[i,j]=1\n",
        "      if i<5:\n",
        "          print(f'input: {\",\".join([str(x) for x in combination[i]])} -> message: {\",\".join([str(x.item()) for x in message])} -> output: {\",\".join([str(x) for x in receiver_outputs[i]])}', flush=True)\n",
        "\n",
        "    unif_acc /= (n_values**n_attributes) * n_attributes\n",
        "\n",
        "    print(json.dumps({'unif': unif_acc}))\n",
        "\n",
        "    return acc_vec, messages\n",
        "\n",
        "def custom_dump_compositionality(game, n_attributes, n_values, device, gs_mode,epoch):\n",
        "\n",
        "    dataset=DistributionUniformLoader(n_values)\n",
        "\n",
        "    sender_inputs, messages, receiver_inputs, receiver_outputs, _ = \\\n",
        "        dump_sender_receiver_compositionality(game, dataset, gs=gs_mode, device=device, variable_length=True)\n",
        "\n",
        "    unif_acc = 0.\n",
        "    acc_vec=np.zeros(((n_values**n_attributes), n_attributes))\n",
        "\n",
        "    for i in range(len(receiver_outputs)):\n",
        "      message=messages[i]\n",
        "      correct=True\n",
        "      if i<n_values**n_attributes:\n",
        "          for j in range(len(distribution_lbl[i])):\n",
        "            if receiver_outputs[i][j]==list(distribution_lbl[i])[j]:\n",
        "              unif_acc+=1\n",
        "              acc_vec[i,j]=1\n",
        "      if i<5:\n",
        "          print(f'input: {\",\".join([str(x) for x in distribution_lbl[i]])} -> message: {\",\".join([str(x.item()) for x in message])} -> output: {\",\".join([str(x) for x in receiver_outputs[i]])}', flush=True)\n",
        "\n",
        "    unif_acc /= (n_values**n_attributes) * n_attributes\n",
        "\n",
        "    print(json.dumps({'unif': unif_acc}))\n",
        "\n",
        "    return acc_vec, messages\n",
        "\n",
        "def dump_impatient_compositionality(game, n_attributes, n_values, device, gs_mode,epoch):\n",
        "    # tiny \"dataset\"\n",
        "    one_hots = torch.eye(n_values)\n",
        "\n",
        "    val=np.arange(n_values)\n",
        "    combination=list(itertools.product(val,repeat=n_attributes))\n",
        "\n",
        "    dataset=[]\n",
        "\n",
        "    for i in range(len(combination)):\n",
        "      new_input=torch.zeros(0)\n",
        "      for j in combination[i]:\n",
        "        new_input=torch.cat((new_input,one_hots[j]))\n",
        "      dataset.append(new_input)\n",
        "\n",
        "    dataset=torch.stack(dataset)\n",
        "\n",
        "    dataset=[[dataset,None]]\n",
        "\n",
        "    sender_inputs, messages, receiver_inputs, receiver_outputs, _ = \\\n",
        "        dump_sender_receiver_impatient_compositionality(game, dataset, gs=gs_mode, device=device, variable_length=True)\n",
        "\n",
        "    unif_acc = 0.\n",
        "    acc_vec=np.zeros(((n_values**n_attributes), n_attributes))\n",
        "\n",
        "    for i in range(len(receiver_outputs)):\n",
        "      message=messages[i]\n",
        "      correct=True\n",
        "      for j in range(len(list(combination[i]))):\n",
        "        if receiver_outputs[i][j]==list(combination[i])[j]:\n",
        "          unif_acc+=1\n",
        "          acc_vec[i,j]=1\n",
        "      if epoch%5==0 and i<5:\n",
        "          print(f'input: {\",\".join([str(x) for x in combination[i]])} -> message: {\",\".join([str(x.item()) for x in message])} -> output: {\",\".join([str(x) for x in receiver_outputs[i]])}', flush=True)\n",
        "\n",
        "    unif_acc /= (n_values**n_attributes) * n_attributes\n",
        "\n",
        "    print(json.dumps({'unif': unif_acc}))\n",
        "\n",
        "    return acc_vec, messages\n",
        "\n",
        "def custom_dump_impatient_compositionality(game, n_attributes, n_values, device, gs_mode,epoch):\n",
        "\n",
        "    dataset = DistributionUniformLoader(n_values)\n",
        "\n",
        "    sender_inputs, messages, receiver_inputs, receiver_outputs, _ = \\\n",
        "        dump_sender_receiver_impatient_compositionality(game, dataset, gs=gs_mode, device=device, variable_length=True)\n",
        "\n",
        "    unif_acc = 0.\n",
        "    acc_vec=np.zeros(((n_values**n_attributes), n_attributes))\n",
        "\n",
        "    for i in range(len(receiver_outputs)):\n",
        "      message=messages[i]\n",
        "      correct=True\n",
        "      for j in range(len(receiver_outputs[i])):\n",
        "        if receiver_outputs[i][j]==list(distribution_lbl[i])[j]:\n",
        "          unif_acc+=1\n",
        "          acc_vec[i,j]=1\n",
        "      if epoch%5==0 and i<5:\n",
        "          print(f'input: {\",\".join([str(x) for x in distribution_lbl[i]])} -> message: {\",\".join([str(x.item()) for x in message])} -> output: {\",\".join([str(x) for x in receiver_outputs[i]])}', flush=True)\n",
        "\n",
        "    unif_acc /= (n_values**n_attributes) * n_attributes\n",
        "\n",
        "    print(json.dumps({'unif': unif_acc}))\n",
        "\n",
        "    return acc_vec, messages"
      ],
      "metadata": {
        "id": "aBduJAj-9K2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline from the git"
      ],
      "metadata": {
        "id": "DpFDy2Fw6prA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy_dict(freq_table):\n",
        "    H = 0\n",
        "    n = sum(v for v in freq_table.values())\n",
        "\n",
        "    for m, freq in freq_table.items():\n",
        "        p = freq_table[m] / n\n",
        "        H += -p * np.log(p)\n",
        "    return H / np.log(2)\n",
        "\n",
        "def _hashable_tensor(t):\n",
        "    if isinstance(t, tuple):\n",
        "        return t\n",
        "    if isinstance(t, int):\n",
        "        return t\n",
        "\n",
        "    try:\n",
        "        t = t.item()\n",
        "    except ValueError:\n",
        "        t = tuple(t.view(-1).tolist())\n",
        "    return t\n",
        "\n",
        "def entropy(messages):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    freq_table = defaultdict(float)\n",
        "\n",
        "    for m in messages:\n",
        "        m = _hashable_tensor(m)\n",
        "        freq_table[m] += 1.0\n",
        "\n",
        "    return entropy_dict(freq_table)\n",
        "\n",
        "def mutual_info(xs, ys):\n",
        "    e_x = entropy(xs)\n",
        "    e_y = entropy(ys)\n",
        "\n",
        "    xys = []\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "        xy = (_hashable_tensor(x), _hashable_tensor(y))\n",
        "        xys.append(xy)\n",
        "\n",
        "    e_xy = entropy(xys)\n",
        "\n",
        "    return e_x + e_y - e_xy"
      ],
      "metadata": {
        "id": "LR6YsCh8OXhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "from egg.core.util import move_to\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sender_input: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        receiver_input: Optional[torch.Tensor] = None,\n",
        "        aux_input: Optional[Dict[Any, Any]] = None,\n",
        "    ):\n",
        "        self.sender_input = sender_input\n",
        "        self.labels = labels\n",
        "        self.receiver_input = receiver_input\n",
        "        self.aux_input = aux_input\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        >>> b = Batch(torch.Tensor([1]), torch.Tensor([2]), torch.Tensor([3]), {})\n",
        "        >>> b[0]\n",
        "        tensor([1.])\n",
        "        >>> b[1]\n",
        "        tensor([2.])\n",
        "        >>> b[2]\n",
        "        tensor([3.])\n",
        "        >>> b[3]\n",
        "        {}\n",
        "        >>> b[6]\n",
        "        Traceback (most recent call last):\n",
        "            ...\n",
        "        IndexError: Trying to access a wrong index in the batch\n",
        "        \"\"\"\n",
        "        if idx == 0:\n",
        "            return self.sender_input\n",
        "        elif idx == 1:\n",
        "            return self.labels\n",
        "        elif idx == 2:\n",
        "            return self.receiver_input\n",
        "        elif idx == 3:\n",
        "            return self.aux_input\n",
        "        else:\n",
        "            raise IndexError(\"Trying to access a wrong index in the batch\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        >>> _ = torch.manual_seed(111)\n",
        "        >>> sender_input = torch.rand(2, 2)\n",
        "        >>> labels = torch.rand(2, 2)\n",
        "        >>> batch = Batch(sender_input, labels)\n",
        "        >>> it = batch.__iter__()\n",
        "        >>> it_sender_input = next(it)\n",
        "        >>> torch.allclose(sender_input, it_sender_input)\n",
        "        True\n",
        "        >>> it_labels = next(it)\n",
        "        >>> torch.allclose(labels, it_labels)\n",
        "        True\n",
        "        \"\"\"\n",
        "        return iter(\n",
        "            [self.sender_input, self.labels, self.receiver_input, self.aux_input]\n",
        "        )\n",
        "\n",
        "    def to(self, device: torch.device):\n",
        "        \"\"\"Method to move all (nested) tensors of the batch to a specific device.\n",
        "        This operation doest not change the original batch element and returns a new Batch instance.\n",
        "        \"\"\"\n",
        "        self.sender_input = move_to(self.sender_input, device)\n",
        "        self.labels = move_to(self.labels, device)\n",
        "        self.receiver_input = move_to(self.receiver_input, device)\n",
        "        self.aux_input = move_to(self.aux_input, device)\n",
        "        return self"
      ],
      "metadata": {
        "id": "Mh7SuJepPpAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u0NovWolKdH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import torch\n",
        "from scipy import spatial\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import egg.core as core\n",
        "\n",
        "try:\n",
        "    import editdistance  # package to install https://pypi.org/project/editdistance/0.3.1/\n",
        "except ImportError:\n",
        "    print(\n",
        "        \"Please install editdistance package: `pip install editdistance`. \"\n",
        "        \"It is used for calculating topographic similarity.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def ask_sender(n_attributes, n_values, dataset, sender, device):\n",
        "    attributes = []\n",
        "    strings = []\n",
        "    meanings = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        meaning = dataset[i]\n",
        "\n",
        "        attribute = meaning.view(n_attributes, n_values).argmax(dim=-1)\n",
        "        attributes.append(attribute)\n",
        "        meanings.append(meaning.to(device))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            string, *other = sender(meaning.unsqueeze(0).to(device))\n",
        "        strings.append(string.squeeze(0))\n",
        "\n",
        "    attributes = torch.stack(attributes, dim=0)\n",
        "    strings = torch.stack(strings, dim=0)\n",
        "    meanings = torch.stack(meanings, dim=0)\n",
        "\n",
        "    return attributes, strings, meanings\n",
        "\n",
        "\n",
        "def information_gap_representation(meanings, representations):\n",
        "    gaps = torch.zeros(representations.size(1))\n",
        "    non_constant_positions = 0.0\n",
        "\n",
        "    for j in range(representations.size(1)):\n",
        "        symbol_mi = []\n",
        "        h_j = None\n",
        "        for i in range(meanings.size(1)):\n",
        "            x, y = meanings[:, i], representations[:, j]\n",
        "            info = mutual_info(x, y)\n",
        "            symbol_mi.append(info)\n",
        "\n",
        "            if h_j is None:\n",
        "                h_j = entropy(y)\n",
        "\n",
        "        symbol_mi.sort(reverse=True)\n",
        "\n",
        "        if h_j > 0.0:\n",
        "            gaps[j] = (symbol_mi[0] - symbol_mi[1]) / h_j\n",
        "            non_constant_positions += 1\n",
        "\n",
        "    score = gaps.sum() / non_constant_positions\n",
        "    return score.item()\n",
        "\n",
        "\n",
        "def information_gap_position(n_attributes, n_values, dataset, sender, device):\n",
        "    attributes, strings, _meanings = ask_sender(\n",
        "        n_attributes, n_values, dataset, sender, device\n",
        "    )\n",
        "    return information_gap_representation(attributes, strings)\n",
        "\n",
        "\n",
        "def histogram(strings, vocab_size):\n",
        "    batch_size = strings.size(0)\n",
        "\n",
        "    histogram = torch.zeros(batch_size, vocab_size, device=strings.device)\n",
        "\n",
        "    for v in range(vocab_size):\n",
        "        histogram[:, v] = strings.eq(v).sum(dim=-1)\n",
        "\n",
        "    return histogram\n",
        "\n",
        "\n",
        "def information_gap_vocab(n_attributes, n_values, dataset, sender, device, vocab_size):\n",
        "    attributes, strings, _meanings = ask_sender(\n",
        "        n_attributes, n_values, dataset, sender, device\n",
        "    )\n",
        "\n",
        "    histograms = histogram(strings, vocab_size)\n",
        "    return information_gap_representation(attributes, histograms[:, 1:])\n",
        "\n",
        "\n",
        "def edit_dist(_list):\n",
        "    distances = []\n",
        "    count = 0\n",
        "    for i, el1 in enumerate(_list[:-1]):\n",
        "        for j, el2 in enumerate(_list[i + 1 :]):\n",
        "            count += 1\n",
        "            # Normalized edit distance (same in our case as length is fixed)\n",
        "            distances.append(editdistance.eval(el1, el2) / len(el1))\n",
        "    return distances\n",
        "\n",
        "\n",
        "def cosine_dist(_list):\n",
        "    distances = []\n",
        "    for i, el1 in enumerate(_list[:-1]):\n",
        "        for j, el2 in enumerate(_list[i + 1 :]):\n",
        "            distances.append(spatial.distance.cosine(el1, el2))\n",
        "    return distances\n",
        "\n",
        "\n",
        "def topographic_similarity(n_attributes, n_values, dataset, sender, device):\n",
        "    _attributes, strings, meanings = ask_sender(\n",
        "        n_attributes, n_values, dataset, sender, device\n",
        "    )\n",
        "    list_string = []\n",
        "    for s in strings:\n",
        "        list_string.append([x.item() for x in s])\n",
        "    distance_messages = edit_dist(list_string)\n",
        "    distance_inputs = cosine_dist(meanings.cpu().numpy())\n",
        "\n",
        "    corr = spearmanr(distance_messages, distance_inputs).correlation\n",
        "    return corr\n",
        "\n",
        "\n",
        "class Metrics(core.Callback):\n",
        "    def __init__(self, dataset, device, n_attributes, n_values, vocab_size, freq=1):\n",
        "        self.dataset = dataset\n",
        "        self.device = device\n",
        "        self.n_attributes = n_attributes\n",
        "        self.n_values = n_values\n",
        "        self.epoch = 0\n",
        "        self.vocab_size = vocab_size\n",
        "        self.freq = freq\n",
        "\n",
        "    def dump_stats(self):\n",
        "        game = self.trainer.game\n",
        "        game.eval()\n",
        "\n",
        "        positional_disent = information_gap_position(\n",
        "            self.n_attributes, self.n_values, self.dataset, game.sender, self.device\n",
        "        )\n",
        "        bos_disent = information_gap_vocab(\n",
        "            self.n_attributes,\n",
        "            self.n_values,\n",
        "            self.dataset,\n",
        "            game.sender,\n",
        "            self.device,\n",
        "            self.vocab_size,\n",
        "        )\n",
        "        topo_sim = topographic_similarity(\n",
        "            self.n_attributes, self.n_values, self.dataset, game.sender, self.device\n",
        "        )\n",
        "\n",
        "        output = dict(\n",
        "            epoch=self.epoch,\n",
        "            positional_disent=positional_disent,\n",
        "            bag_of_symbol_disent=bos_disent,\n",
        "            topographic_sim=topo_sim,\n",
        "        )\n",
        "\n",
        "        output_json = json.dumps(output)\n",
        "        print(output_json, flush=True)\n",
        "\n",
        "        game.train()\n",
        "\n",
        "    def on_train_end(self):\n",
        "        pass\n",
        "        #self.dump_stats()\n",
        "\n",
        "    def on_epoch_end(self, *stuff):\n",
        "        self.epoch += 1\n",
        "\n",
        "        if self.freq <= 0 or self.epoch % self.freq != 0:\n",
        "            return\n",
        "\n",
        "        self.dump_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ucGSZirZ223H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())\n",
        "opts = Config()\n",
        "import egg\n",
        "egg.core.util.common_opts = opts\n",
        "print(opts, flush=True)\n",
        "device = opts.device\n",
        "\n",
        "force_eos = opts.force_eos == 1\n",
        "\n",
        "# Distribution of the inputs\n",
        "if opts.probs==\"uniform\":\n",
        "    probs=[]\n",
        "    probs_by_att = np.ones(opts.n_values)\n",
        "    probs_by_att /= probs_by_att.sum()\n",
        "    for i in range(opts.n_attributes):\n",
        "        probs.append(probs_by_att)\n",
        "\n",
        "if opts.probs==\"entropy_test\":\n",
        "    probs=[]\n",
        "    for i in range(opts.n_attributes):\n",
        "        probs_by_att = np.ones(opts.n_values)\n",
        "        probs_by_att[0]=1+(1*i)\n",
        "        probs_by_att /= probs_by_att.sum()\n",
        "        probs.append(probs_by_att)\n",
        "\n",
        "if opts.probs_attributes==\"uniform\":\n",
        "    probs_attributes=[1]*opts.n_attributes\n",
        "\n",
        "if opts.probs_attributes==\"uniform_indep\":\n",
        "    probs_attributes=[]\n",
        "    probs_attributes=[0.2]*opts.n_attributes\n",
        "\n",
        "if opts.probs_attributes==\"echelon\":\n",
        "    probs_attributes=[]\n",
        "    for i in range(opts.n_attributes):\n",
        "        #probs_attributes.append(1.-(0.2)*i)\n",
        "        #probs_attributes.append(0.7+0.3/(i+1))\n",
        "        probs_attributes=[1.,0.95,0.9,0.85]\n",
        "\n",
        "print(\"Probability by attribute is:\",probs_attributes)\n",
        "\n",
        "if opts.custom_dist:\n",
        "    train_loader = DistributionLoader(n_features=opts.n_values, batch_size=opts.batch_size,batches_per_epoch=opts.batches_per_epoch)\n",
        "    #OneHotLoaderCompositionality\n",
        "    # single batches with 1s on the diag\n",
        "    test_loader = DistributionUniformLoader(n_features=opts.n_values)\n",
        "else:\n",
        "    #TestLoaderCompositionality\n",
        "    train_loader = OneHotLoaderCompositionality(n_values=opts.n_values, n_attributes=opts.n_attributes, batch_size=opts.batch_size*opts.n_attributes,\n",
        "                                                batches_per_epoch=opts.batches_per_epoch, probs=probs, probs_attributes=probs_attributes)\n",
        "\n",
        "    # single batches with 1s on the diag\n",
        "    test_loader = TestLoaderCompositionality(n_values=opts.n_values,n_attributes=opts.n_attributes)\n",
        "### SENDER ###\n",
        "\n",
        "sender = Sender(n_features=opts.n_attributes*opts.n_values, n_hidden=opts.sender_hidden)\n",
        "\n",
        "sender = core.RnnSenderReinforce(sender,opts.vocab_size, opts.sender_embedding, opts.sender_hidden,\n",
        "                                cell=opts.sender_cell, max_len=opts.max_len, num_layers=opts.sender_num_layers,\n",
        "                                force_eos=force_eos)\n",
        "\n",
        "\n",
        "### RECEIVER ###\n",
        "\n",
        "receiver = Receiver(n_features=opts.n_values, n_hidden=opts.receiver_hidden)\n",
        "\n",
        "if not opts.impatient:\n",
        "    receiver = Receiver(n_features=opts.n_features, n_hidden=opts.receiver_hidden)\n",
        "    receiver = RnnReceiverCompositionality(receiver, opts.vocab_size, opts.receiver_embedding,\n",
        "                                        opts.receiver_hidden, cell=opts.receiver_cell,\n",
        "                                        num_layers=opts.receiver_num_layers, max_len=opts.max_len, n_attributes=opts.n_attributes, n_values=opts.n_values)\n",
        "else:\n",
        "    receiver = Receiver(n_features=opts.receiver_hidden, n_hidden=opts.vocab_size)\n",
        "    # If impatient 1\n",
        "    receiver = RnnReceiverImpatientCompositionality(receiver, opts.vocab_size, opts.receiver_embedding,\n",
        "                                        opts.receiver_hidden, cell=opts.receiver_cell,\n",
        "                                        num_layers=opts.receiver_num_layers, max_len=opts.max_len, n_attributes=opts.n_attributes, n_values=opts.n_values)\n",
        "\n",
        "\n",
        "if not opts.impatient:\n",
        "    game = CompositionalitySenderReceiverRnnReinforce(sender, receiver, loss_compositionality, sender_entropy_coeff=opts.sender_entropy_coeff,\n",
        "                                        n_attributes=opts.n_attributes,n_values=opts.n_values,receiver_entropy_coeff=opts.receiver_entropy_coeff,\n",
        "                                        length_cost=opts.length_cost,unigram_penalty=opts.unigram_pen,reg=opts.reg)\n",
        "else:\n",
        "    game = CompositionalitySenderImpatientReceiverRnnReinforce(sender, receiver, loss_impatient_compositionality, sender_entropy_coeff=opts.sender_entropy_coeff,\n",
        "                                        n_attributes=opts.n_attributes,n_values=opts.n_values,att_weights=opts.att_weights,receiver_entropy_coeff=opts.receiver_entropy_coeff,\n",
        "                                        length_cost=opts.length_cost,unigram_penalty=opts.unigram_pen,reg=opts.reg)\n",
        "\n",
        "\n",
        "optimizer = opts.optimizer_class(game.parameters(),lr=opts.lr)\n",
        "\n",
        "#Create dataset for metric computation\n",
        "if opts.custom_dist:\n",
        "    #Dataset creation\n",
        "    idxs = np.arange(len(distribution))\n",
        "    batch_data = np.zeros((len(distribution),opts.n_values*2,))\n",
        "    for i in range(distribution_lbl.shape[1]):\n",
        "        batch_data[np.arange(len(distribution)),distribution_lbl[idxs,i]+i*opts.n_values] = 1\n",
        "    dataset = torch.from_numpy(batch_data).float()\n",
        "else:\n",
        "    l = []\n",
        "    c = 0\n",
        "    data = next(iter(test_loader))[0]\n",
        "    for idx,i in enumerate(data):\n",
        "        c += 1\n",
        "        if idx < 2000:\n",
        "            l.append(i[None])\n",
        "    dataset = torch.cat(l,axis=0)\n",
        "    print(\"metric dataset\",dataset.shape)\n",
        "\n",
        "trainer = CompoTrainer(n_attributes=opts.n_attributes,n_values=opts.n_values,game=game, optimizer=optimizer, train_data=train_loader,\n",
        "                        validation_data=test_loader, callbacks=[EarlyStopperAccuracy(opts.early_stopping_thr),\n",
        "                                                                Metrics(dataset, opts.device, opts.n_attributes, opts.n_values, opts.vocab_size, freq=20)])\n",
        "\n",
        "curr_accs=[0]*7\n",
        "\n",
        "game.att_weights=[1]*(game.n_attributes)\n",
        "\n"
      ],
      "metadata": {
        "id": "qHWtFn3KNoSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7287f0e-0abe-45cf-a879-a2de11cec08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "<__main__.Config object at 0x7f805baa6910>\n",
            "Probability by attribute is: [1, 1]\n",
            "metric dataset torch.Size([2000, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(int(opts.n_epochs)):\n",
        "\n",
        "    print(\"Epoch: \"+str(epoch))\n",
        "\n",
        "    #if epoch%100==0:\n",
        "    #  trainer.optimizer.defaults[\"lr\"]/=2\n",
        "\n",
        "\n",
        "    trainer.train(n_epochs=1)\n",
        "    if opts.checkpoint_dir:\n",
        "        trainer.save_checkpoint(name=f'{opts.name}_vocab{opts.vocab_size}_rs{opts.random_seed}_lr{opts.lr}_shid{opts.sender_hidden}_rhid{opts.receiver_hidden}_sentr{opts.sender_entropy_coeff}_reg{opts.length_cost}_max_len{opts.max_len}')\n",
        "\n",
        "    if not opts.impatient:\n",
        "        if opts.custom_dist:\n",
        "            acc_vec,messages=custom_dump_compositionality(trainer.game, opts.n_attributes, opts.n_values, device, False,epoch)\n",
        "        else:\n",
        "            acc_vec,messages=dump_compositionality(trainer.game, opts.n_attributes, opts.n_values, device, False,epoch)\n",
        "    else:\n",
        "        if opts.custom_dist:\n",
        "            acc_vec,messages=custom_dump_impatient_compositionality(trainer.game, opts.n_attributes, opts.n_values, device, False,epoch)\n",
        "        else:\n",
        "            acc_vec,messages=dump_impatient_compositionality(trainer.game, opts.n_attributes, opts.n_values, device, False,epoch)\n",
        "\n",
        "    #print(acc_vec.mean(0))\n",
        "    #print(trainer.optimizer.defaults[\"lr\"])\n",
        "\n",
        "\n",
        "    # ADDITION TO SAVE MESSAGES\n",
        "    all_messages=[]\n",
        "    for x in messages:\n",
        "        x = x.cpu().numpy()\n",
        "        all_messages.append(x)\n",
        "    all_messages = np.asarray(all_messages)\n",
        "\n",
        "    if epoch%50==0:\n",
        "        torch.save(sender.state_dict(), opts.dir_save+\"/sender/sender_weights\"+str(epoch)+\".pth\")\n",
        "        torch.save(receiver.state_dict(), opts.dir_save+\"/receiver/receiver_weights\"+str(epoch)+\".pth\")\n",
        "\n",
        "    np.save(opts.dir_save+'/messages/messages_'+str((epoch))+'.npy', all_messages)\n",
        "    np.save(opts.dir_save+'/accuracy/accuracy_'+str((epoch))+'.npy', acc_vec)\n",
        "    print(acc_vec.T)\n",
        "\n",
        "core.close()\n"
      ],
      "metadata": {
        "id": "W6-HCBvsd-iA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b761fd-6085-44c8-be93-d5ba2e36795d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "0\n",
            "Train loss: 2.7147512435913086\n",
            "Eval loss: 11.284082412719727\n",
            "{'acc': 0.13124999403953552, 'loss': 11.284082412719727, 'sender_entropy': 4.577314376831055, 'receiver_entropy': 4.553857803344727, 'original_loss': 8.465066909790039, 'mean_length': 1.9749999046325684}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/egg/core/util.py:847: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  eos_positions = (message[i, :] == 0).nonzero()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 0,0 -> message: 80,80 -> output: 0,29\n",
            "input: 0,1 -> message: 80,80 -> output: 0,29\n",
            "input: 0,2 -> message: 80,80 -> output: 0,29\n",
            "input: 0,3 -> message: 41,41 -> output: 23,34\n",
            "input: 0,4 -> message: 80,80 -> output: 0,29\n",
            "{\"unif\": 0.13125}\n",
            "[[1. 1. 1. ... 1. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch: 1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 11.569842338562012\n",
            "Eval loss: 23.469623565673828\n",
            "{'acc': 0.3035999834537506, 'loss': 23.469623565673828, 'sender_entropy': 3.418201446533203, 'receiver_entropy': 3.228217840194702, 'original_loss': 5.426722526550293, 'mean_length': 1.9994999170303345}\n",
            "input: 0,0 -> message: 41,41 -> output: 0,14\n",
            "input: 0,1 -> message: 22,41 -> output: 0,25\n",
            "input: 0,2 -> message: 49,41 -> output: 0,90\n",
            "input: 0,3 -> message: 22,41 -> output: 0,25\n",
            "input: 0,4 -> message: 22,41 -> output: 0,25\n",
            "{\"unif\": 0.3036}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch: 2\n",
            "0\n",
            "Train loss: 18.29550552368164\n",
            "Eval loss: 19.17620086669922\n",
            "{'acc': 0.4197499752044678, 'loss': 19.17620086669922, 'sender_entropy': 2.609553813934326, 'receiver_entropy': 2.5205986499786377, 'original_loss': 4.514237403869629, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 42,41 -> output: 0,85\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 49,41 -> output: 0,90\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 54,41 -> output: 0,3\n",
            "{\"unif\": 0.41975}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch: 3\n",
            "0\n",
            "Train loss: 17.731916427612305\n",
            "Eval loss: 16.733396530151367\n",
            "{'acc': 0.47154998779296875, 'loss': 16.733396530151367, 'sender_entropy': 2.2002177238464355, 'receiver_entropy': 2.2179644107818604, 'original_loss': 3.9814980030059814, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 85,41 -> output: 23,6\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 36,41 -> output: 23,92\n",
            "input: 0,3 -> message: 94,41 -> output: 23,89\n",
            "input: 0,4 -> message: 36,41 -> output: 23,92\n",
            "{\"unif\": 0.47155}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "Epoch: 4\n",
            "0\n",
            "Train loss: 15.700728416442871\n",
            "Eval loss: 14.878854751586914\n",
            "{'acc': 0.5307999849319458, 'loss': 14.878854751586914, 'sender_entropy': 1.9472858905792236, 'receiver_entropy': 2.00657057762146, 'original_loss': 3.535792827606201, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 50,41 -> output: 0,29\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 49,41 -> output: 0,1\n",
            "input: 0,3 -> message: 92,41 -> output: 23,62\n",
            "input: 0,4 -> message: 4,41 -> output: 23,48\n",
            "{\"unif\": 0.5308}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [0. 1. 0. ... 1. 1. 0.]]\n",
            "Epoch: 5\n",
            "0\n",
            "Train loss: 13.80666446685791\n",
            "Eval loss: 13.268494606018066\n",
            "{'acc': 0.569350004196167, 'loss': 13.268494606018066, 'sender_entropy': 1.7585487365722656, 'receiver_entropy': 1.8122780323028564, 'original_loss': 3.0685031414031982, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 50,41 -> output: 23,45\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 76,41 -> output: 0,90\n",
            "input: 0,3 -> message: 70,41 -> output: 0,4\n",
            "input: 0,4 -> message: 4,41 -> output: 0,90\n",
            "{\"unif\": 0.56935}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch: 6\n",
            "0\n",
            "Train loss: 12.173776626586914\n",
            "Eval loss: 11.379423141479492\n",
            "{'acc': 0.6070500016212463, 'loss': 11.379423141479492, 'sender_entropy': 1.461424708366394, 'receiver_entropy': 1.5360828638076782, 'original_loss': 2.4883365631103516, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 91,41 -> output: 0,13\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 76,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,73\n",
            "input: 0,4 -> message: 92,41 -> output: 0,36\n",
            "{\"unif\": 0.60705}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "Epoch: 7\n",
            "0\n",
            "Train loss: 10.605327606201172\n",
            "Eval loss: 9.479036331176758\n",
            "{'acc': 0.6568999886512756, 'loss': 9.479036331176758, 'sender_entropy': 1.1923997402191162, 'receiver_entropy': 1.2870972156524658, 'original_loss': 2.011577606201172, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 14,41 -> output: 23,67\n",
            "input: 0,1 -> message: 76,41 -> output: 23,66\n",
            "input: 0,2 -> message: 76,41 -> output: 23,66\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 29,41 -> output: 23,87\n",
            "{\"unif\": 0.6569}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 0. 0.]]\n",
            "Epoch: 8\n",
            "0\n",
            "Train loss: 8.754328727722168\n",
            "Eval loss: 6.893385887145996\n",
            "{'acc': 0.7684999704360962, 'loss': 6.893385887145996, 'sender_entropy': 0.8347802758216858, 'receiver_entropy': 0.9603109955787659, 'original_loss': 1.3565064668655396, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 3,41 -> output: 23,17\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 76,41 -> output: 0,66\n",
            "input: 0,3 -> message: 54,41 -> output: 0,73\n",
            "input: 0,4 -> message: 92,41 -> output: 23,83\n",
            "{\"unif\": 0.7685}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch: 9\n",
            "0\n",
            "Train loss: 5.529320240020752\n",
            "Eval loss: 2.875946283340454\n",
            "{'acc': 0.8904999494552612, 'loss': 2.875946283340454, 'sender_entropy': 0.3196219503879547, 'receiver_entropy': 0.45661112666130066, 'original_loss': 0.5275492072105408, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 52,41 -> output: 23,50\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.8905}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 10\n",
            "0\n",
            "Train loss: 1.8613024950027466\n",
            "Eval loss: 1.0404897928237915\n",
            "{'acc': 0.9256500005722046, 'loss': 1.0404897928237915, 'sender_entropy': 0.0753316879272461, 'receiver_entropy': 0.1744873821735382, 'original_loss': 0.2403799295425415, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.92565}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 11\n",
            "0\n",
            "Train loss: 0.9319940209388733\n",
            "Eval loss: 0.7537394165992737\n",
            "{'acc': 0.9302499890327454, 'loss': 0.7537394165992737, 'sender_entropy': 0.052158456295728683, 'receiver_entropy': 0.13449177145957947, 'original_loss': 0.21622399985790253, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93025}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 12\n",
            "0\n",
            "Train loss: 0.7553155422210693\n",
            "Eval loss: 0.6655384302139282\n",
            "{'acc': 0.9302999973297119, 'loss': 0.6655384302139282, 'sender_entropy': 0.04430850222706795, 'receiver_entropy': 0.12000375986099243, 'original_loss': 0.20979687571525574, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9303}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 13\n",
            "0\n",
            "Train loss: 0.6740031838417053\n",
            "Eval loss: 0.6140141487121582\n",
            "{'acc': 0.9303500056266785, 'loss': 0.6140141487121582, 'sender_entropy': 0.04175679385662079, 'receiver_entropy': 0.11446920037269592, 'original_loss': 0.20667299628257751, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93035}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 14\n",
            "0\n",
            "Train loss: 0.6381239295005798\n",
            "Eval loss: 0.5763698220252991\n",
            "{'acc': 0.9303500056266785, 'loss': 0.5763698220252991, 'sender_entropy': 0.04314151033759117, 'receiver_entropy': 0.10999596863985062, 'original_loss': 0.20529992878437042, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93035}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 15\n",
            "0\n",
            "Train loss: 0.5899451971054077\n",
            "Eval loss: 0.5268795490264893\n",
            "{'acc': 0.9301999807357788, 'loss': 0.5268795490264893, 'sender_entropy': 0.04300791770219803, 'receiver_entropy': 0.10831845551729202, 'original_loss': 0.20480799674987793, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9302}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 16\n",
            "0\n",
            "Train loss: 0.5633825063705444\n",
            "Eval loss: 0.5108378529548645\n",
            "{'acc': 0.9301999807357788, 'loss': 0.5108378529548645, 'sender_entropy': 0.04403015598654747, 'receiver_entropy': 0.10598190873861313, 'original_loss': 0.20474524796009064, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9302}\n",
            "[[0. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 17\n",
            "0\n",
            "Train loss: 0.5361167788505554\n",
            "Eval loss: 0.4768560528755188\n",
            "{'acc': 0.9302499890327454, 'loss': 0.4768560528755188, 'sender_entropy': 0.042063917964696884, 'receiver_entropy': 0.10727030038833618, 'original_loss': 0.20355893671512604, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93025}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 18\n",
            "0\n",
            "Train loss: 0.5104157328605652\n",
            "Eval loss: 0.4777927100658417\n",
            "{'acc': 0.9301999807357788, 'loss': 0.4777927100658417, 'sender_entropy': 0.04432905092835426, 'receiver_entropy': 0.10508199036121368, 'original_loss': 0.20338210463523865, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9302}\n",
            "[[1. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 19\n",
            "0\n",
            "Train loss: 0.4902481138706207\n",
            "{\"epoch\": 20, \"positional_disent\": 0.9750906825065613, \"bag_of_symbol_disent\": 0.86342853307724, \"topographic_sim\": 0.9641215816048193}\n",
            "Eval loss: 0.45527344942092896\n",
            "{'acc': 0.9301499724388123, 'loss': 0.45527344942092896, 'sender_entropy': 0.04284762591123581, 'receiver_entropy': 0.10577110201120377, 'original_loss': 0.20328031480312347, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93015}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 20\n",
            "0\n",
            "Train loss: 0.47179046273231506\n",
            "Eval loss: 0.4341844916343689\n",
            "{'acc': 0.9303500056266785, 'loss': 0.4341844916343689, 'sender_entropy': 0.043732061982154846, 'receiver_entropy': 0.1042550727725029, 'original_loss': 0.20386789739131927, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93035}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 21\n",
            "0\n",
            "Train loss: 0.4548819065093994\n",
            "Eval loss: 0.42482680082321167\n",
            "{'acc': 0.9302999973297119, 'loss': 0.42482680082321167, 'sender_entropy': 0.043982695788145065, 'receiver_entropy': 0.1044657900929451, 'original_loss': 0.203460231423378, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9303}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 22\n",
            "0\n",
            "Train loss: 0.44082266092300415\n",
            "Eval loss: 0.40651535987854004\n",
            "{'acc': 0.9304499626159668, 'loss': 0.40651535987854004, 'sender_entropy': 0.043732646852731705, 'receiver_entropy': 0.10385013371706009, 'original_loss': 0.20226550102233887, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93045}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 23\n",
            "0\n",
            "Train loss: 0.4234964847564697\n",
            "Eval loss: 0.38243570923805237\n",
            "{'acc': 0.9302499890327454, 'loss': 0.38243570923805237, 'sender_entropy': 0.04268932715058327, 'receiver_entropy': 0.10364408791065216, 'original_loss': 0.20265507698059082, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93025}\n",
            "[[1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 24\n",
            "0\n",
            "Train loss: 0.4088987410068512\n",
            "Eval loss: 0.3847010135650635\n",
            "{'acc': 0.9303500056266785, 'loss': 0.3847010135650635, 'sender_entropy': 0.0442139208316803, 'receiver_entropy': 0.10323356837034225, 'original_loss': 0.20278078317642212, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93035}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 25\n",
            "0\n",
            "Train loss: 0.3962647616863251\n",
            "Eval loss: 0.35587769746780396\n",
            "{'acc': 0.9303999543190002, 'loss': 0.35587769746780396, 'sender_entropy': 0.04301631450653076, 'receiver_entropy': 0.1033729612827301, 'original_loss': 0.20235304534435272, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9304}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 26\n",
            "0\n",
            "Train loss: 0.385850191116333\n",
            "Eval loss: 0.35602545738220215\n",
            "{'acc': 0.9303999543190002, 'loss': 0.35602545738220215, 'sender_entropy': 0.043316107243299484, 'receiver_entropy': 0.10365369915962219, 'original_loss': 0.20179489254951477, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9304}\n",
            "[[1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 27\n",
            "0\n",
            "Train loss: 0.37507086992263794\n",
            "Eval loss: 0.3588774800300598\n",
            "{'acc': 0.9302499890327454, 'loss': 0.3588774800300598, 'sender_entropy': 0.04420933127403259, 'receiver_entropy': 0.10430019348859787, 'original_loss': 0.2024795114994049, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93025}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 28\n",
            "0\n",
            "Train loss: 0.36047986149787903\n",
            "Eval loss: 0.3409995436668396\n",
            "{'acc': 0.9303500056266785, 'loss': 0.3409995436668396, 'sender_entropy': 0.04436010494828224, 'receiver_entropy': 0.10281949490308762, 'original_loss': 0.20203281939029694, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93035}\n",
            "[[0. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 29\n",
            "0\n",
            "Train loss: 0.35266372561454773\n",
            "Eval loss: 0.32850182056427\n",
            "{'acc': 0.9303999543190002, 'loss': 0.32850182056427, 'sender_entropy': 0.04376261308789253, 'receiver_entropy': 0.10292353481054306, 'original_loss': 0.20160770416259766, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9304}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 30\n",
            "0\n",
            "Train loss: 0.34366023540496826\n",
            "Eval loss: 0.3277602791786194\n",
            "{'acc': 0.9303999543190002, 'loss': 0.3277602791786194, 'sender_entropy': 0.04418138414621353, 'receiver_entropy': 0.10376320779323578, 'original_loss': 0.20167890191078186, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9304}\n",
            "[[0. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 31\n",
            "0\n",
            "Train loss: 0.33399683237075806\n",
            "Eval loss: 0.31874963641166687\n",
            "{'acc': 0.9303500056266785, 'loss': 0.31874963641166687, 'sender_entropy': 0.044558800756931305, 'receiver_entropy': 0.10350451618432999, 'original_loss': 0.2011401355266571, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93035}\n",
            "[[1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 32\n",
            "0\n",
            "Train loss: 0.33025923371315\n",
            "Eval loss: 0.3129644989967346\n",
            "{'acc': 0.9302499890327454, 'loss': 0.3129644989967346, 'sender_entropy': 0.044000111520290375, 'receiver_entropy': 0.10408423840999603, 'original_loss': 0.20208275318145752, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93025}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 33\n",
            "0\n",
            "Train loss: 0.32216590642929077\n",
            "Eval loss: 0.30327150225639343\n",
            "{'acc': 0.9303500056266785, 'loss': 0.30327150225639343, 'sender_entropy': 0.04429053142666817, 'receiver_entropy': 0.10282114148139954, 'original_loss': 0.20129123330116272, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93035}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 34\n",
            "0\n",
            "Train loss: 0.3140018880367279\n",
            "Eval loss: 0.29723459482192993\n",
            "{'acc': 0.9303500056266785, 'loss': 0.29723459482192993, 'sender_entropy': 0.04378209263086319, 'receiver_entropy': 0.10290351510047913, 'original_loss': 0.20124885439872742, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93035}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 35\n",
            "0\n",
            "Train loss: 0.30664169788360596\n",
            "Eval loss: 0.29345154762268066\n",
            "{'acc': 0.9304999709129333, 'loss': 0.29345154762268066, 'sender_entropy': 0.044373657554388046, 'receiver_entropy': 0.10295204818248749, 'original_loss': 0.20177632570266724, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9305}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 36\n",
            "0\n",
            "Train loss: 0.3024066090583801\n",
            "Eval loss: 0.2869514226913452\n",
            "{'acc': 0.9303999543190002, 'loss': 0.2869514226913452, 'sender_entropy': 0.04403301700949669, 'receiver_entropy': 0.10298178344964981, 'original_loss': 0.20125418901443481, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9304}\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 37\n",
            "0\n",
            "Train loss: 0.29526442289352417\n",
            "Eval loss: 0.2803179621696472\n",
            "{'acc': 0.9304499626159668, 'loss': 0.2803179621696472, 'sender_entropy': 0.044318586587905884, 'receiver_entropy': 0.10233958810567856, 'original_loss': 0.20113509893417358, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93045}\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 38\n",
            "0\n",
            "Train loss: 0.2905242145061493\n",
            "Eval loss: 0.27678602933883667\n",
            "{'acc': 0.9304999709129333, 'loss': 0.27678602933883667, 'sender_entropy': 0.04461070895195007, 'receiver_entropy': 0.10353683680295944, 'original_loss': 0.2010302096605301, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9305}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 39\n",
            "0\n",
            "Train loss: 0.28513094782829285\n",
            "{\"epoch\": 40, \"positional_disent\": 0.9694733619689941, \"bag_of_symbol_disent\": 0.8512244820594788, \"topographic_sim\": 0.9585012153079325}\n",
            "Eval loss: 0.2741575241088867\n",
            "{'acc': 0.9304499626159668, 'loss': 0.2741575241088867, 'sender_entropy': 0.044357165694236755, 'receiver_entropy': 0.1029021218419075, 'original_loss': 0.20082543790340424, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93045}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 40\n",
            "0\n",
            "Train loss: 0.27821797132492065\n",
            "Eval loss: 0.266410231590271\n",
            "{'acc': 0.9304999709129333, 'loss': 0.266410231590271, 'sender_entropy': 0.04434571415185928, 'receiver_entropy': 0.10249040275812149, 'original_loss': 0.20098985731601715, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9305}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 41\n",
            "0\n",
            "Train loss: 0.27451786398887634\n",
            "Eval loss: 0.26200246810913086\n",
            "{'acc': 0.9304999709129333, 'loss': 0.26200246810913086, 'sender_entropy': 0.04411524534225464, 'receiver_entropy': 0.10299044847488403, 'original_loss': 0.2005893886089325, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9305}\n",
            "[[1. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n",
            "Epoch: 42\n",
            "0\n",
            "Train loss: 0.2688538432121277\n",
            "Eval loss: 0.2610776424407959\n",
            "{'acc': 0.9305499792098999, 'loss': 0.2610776424407959, 'sender_entropy': 0.04494192823767662, 'receiver_entropy': 0.1026269719004631, 'original_loss': 0.20112377405166626, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93055}\n",
            "[[1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 43\n",
            "0\n",
            "Train loss: 0.2645173668861389\n",
            "Eval loss: 0.2548319101333618\n",
            "{'acc': 0.9301999807357788, 'loss': 0.2548319101333618, 'sender_entropy': 0.04463861137628555, 'receiver_entropy': 0.10325538367033005, 'original_loss': 0.20265063643455505, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9302}\n",
            "[[1. 0. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 44\n",
            "0\n",
            "Train loss: 0.26111745834350586\n",
            "Eval loss: 0.2494775354862213\n",
            "{'acc': 0.9304999709129333, 'loss': 0.2494775354862213, 'sender_entropy': 0.04418560862541199, 'receiver_entropy': 0.10257673263549805, 'original_loss': 0.20066291093826294, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9305}\n",
            "[[1. 1. 0. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 45\n",
            "0\n",
            "Train loss: 0.25686511397361755\n",
            "Eval loss: 0.2451661229133606\n",
            "{'acc': 0.9304499626159668, 'loss': 0.2451661229133606, 'sender_entropy': 0.044261038303375244, 'receiver_entropy': 0.10242931544780731, 'original_loss': 0.20052145421504974, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 23,3\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.93045}\n",
            "[[0. 1. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "Epoch: 46\n",
            "0\n",
            "Train loss: 0.25403428077697754\n",
            "Eval loss: 0.2439596951007843\n",
            "{'acc': 0.9303999543190002, 'loss': 0.2439596951007843, 'sender_entropy': 0.044660165905952454, 'receiver_entropy': 0.10272524505853653, 'original_loss': 0.2009422779083252, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9304}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 47\n",
            "0\n",
            "Train loss: 0.2484334409236908\n",
            "Eval loss: 0.23948490619659424\n",
            "{'acc': 0.9304499626159668, 'loss': 0.23948490619659424, 'sender_entropy': 0.04421752318739891, 'receiver_entropy': 0.10242927074432373, 'original_loss': 0.20086775720119476, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 23,0\n",
            "input: 0,1 -> message: 49,41 -> output: 23,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93045}\n",
            "[[0. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 48\n",
            "0\n",
            "Train loss: 0.24554330110549927\n",
            "Eval loss: 0.23424269258975983\n",
            "{'acc': 0.9304999709129333, 'loss': 0.23424269258975983, 'sender_entropy': 0.044226694852113724, 'receiver_entropy': 0.10215781629085541, 'original_loss': 0.2002093642950058, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 23,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 23,4\n",
            "{\"unif\": 0.9305}\n",
            "[[1. 1. 0. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "Epoch: 49\n",
            "0\n",
            "Train loss: 0.24240046739578247\n",
            "Eval loss: 0.23209938406944275\n",
            "{'acc': 0.9304499626159668, 'loss': 0.23209938406944275, 'sender_entropy': 0.04436074197292328, 'receiver_entropy': 0.10235026478767395, 'original_loss': 0.20057664811611176, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,90\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,89\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.93045}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]]\n",
            "Epoch: 50\n",
            "0\n",
            "Train loss: 0.24020498991012573\n",
            "Eval loss: 0.23296165466308594\n",
            "{'acc': 0.9305999875068665, 'loss': 0.23296165466308594, 'sender_entropy': 0.04418649524450302, 'receiver_entropy': 0.10389191657304764, 'original_loss': 0.20071391761302948, 'mean_length': 2.0}\n",
            "input: 0,0 -> message: 15,41 -> output: 0,0\n",
            "input: 0,1 -> message: 49,41 -> output: 0,1\n",
            "input: 0,2 -> message: 60,41 -> output: 0,2\n",
            "input: 0,3 -> message: 54,41 -> output: 0,3\n",
            "input: 0,4 -> message: 24,41 -> output: 0,4\n",
            "{\"unif\": 0.9306}\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates folder with stats\n",
        "!mkdir saved_stats\n",
        "\n",
        "#Selects what to keep\n",
        "!zip -r /content/saved_stats/accuracy.zip /content/dir_save/accuracy\n",
        "!zip -r /content/saved_stats/messages.zip /content/dir_save/messages\n",
        "!mv /content/dir_save/receiver /content/saved_stats/receiver\n",
        "!mv /content/dir_save/sender /content/saved_stats/sender\n",
        "\n",
        "!zip -r /content/saved_stats.zip /content/saved_stats\n",
        "\n",
        "!mv saved_stats.zip saved_stats_2.zip\n",
        "\n",
        "#Downloads the file\n",
        "from google.colab import files\n",
        "files.download(\"/content/saved_stats_2.zip\")"
      ],
      "metadata": {
        "id": "aScu_Ig_0QCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "787a7076-6d8a-4590-caff-c14e7f359a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/dir_save/accuracy/ (stored 0%)\n",
            "  adding: content/dir_save/accuracy/accuracy_11.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_22.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_15.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_17.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_21.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_23.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_39.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_20.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_35.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_8.npy (deflated 97%)\n",
            "  adding: content/dir_save/accuracy/accuracy_48.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_14.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_28.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_43.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_6.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_37.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_30.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_27.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_36.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_19.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_1.npy (deflated 97%)\n",
            "  adding: content/dir_save/accuracy/accuracy_29.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_26.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_44.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_49.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_5.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_3.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_2.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_31.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_7.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_13.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_32.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_16.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_40.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_38.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_41.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_24.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_10.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_9.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_12.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_45.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_18.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_46.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_34.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_33.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_50.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_42.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_4.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_0.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_25.npy (deflated 98%)\n",
            "  adding: content/dir_save/accuracy/accuracy_47.npy (deflated 98%)\n",
            "  adding: content/dir_save/messages/ (stored 0%)\n",
            "  adding: content/dir_save/messages/messages_30.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_48.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_31.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_12.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_34.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_43.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_29.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_0.npy (deflated 76%)\n",
            "  adding: content/dir_save/messages/messages_18.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_36.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_44.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_41.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_37.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_14.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_1.npy (deflated 76%)\n",
            "  adding: content/dir_save/messages/messages_2.npy (deflated 94%)\n",
            "  adding: content/dir_save/messages/messages_46.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_50.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_8.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_39.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_49.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_15.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_7.npy (deflated 90%)\n",
            "  adding: content/dir_save/messages/messages_21.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_25.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_42.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_9.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_5.npy (deflated 91%)\n",
            "  adding: content/dir_save/messages/messages_27.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_4.npy (deflated 92%)\n",
            "  adding: content/dir_save/messages/messages_24.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_40.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_26.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_45.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_35.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_13.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_38.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_3.npy (deflated 92%)\n",
            "  adding: content/dir_save/messages/messages_22.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_10.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_47.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_6.npy (deflated 90%)\n",
            "  adding: content/dir_save/messages/messages_17.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_33.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_32.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_28.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_23.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_11.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_19.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_16.npy (deflated 89%)\n",
            "  adding: content/dir_save/messages/messages_20.npy (deflated 89%)\n",
            "  adding: content/saved_stats/ (stored 0%)\n",
            "  adding: content/saved_stats/receiver/ (stored 0%)\n",
            "  adding: content/saved_stats/receiver/receiver_weights0.pth (deflated 8%)\n",
            "  adding: content/saved_stats/receiver/receiver_weights50.pth (deflated 7%)\n",
            "  adding: content/saved_stats/accuracy.zip (stored 0%)\n",
            "  adding: content/saved_stats/messages.zip (stored 0%)\n",
            "  adding: content/saved_stats/sender/ (stored 0%)\n",
            "  adding: content/saved_stats/sender/sender_weights0.pth (deflated 8%)\n",
            "  adding: content/saved_stats/sender/sender_weights50.pth (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75ddb177-0798-4f64-9e10-6a897c9fcf52\", \"saved_stats_2.zip\", 37319032)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}